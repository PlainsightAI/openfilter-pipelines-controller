# Failure Mode Test Scenarios

This document tracks validation of the failure handling behavior described in `DESIGN_DOCUMENT.md#8-failure-modes--recovery`. Each scenario lists the design expectation, reproduction steps, observed behavior, and whether the implementation currently matches the design.

| ID | Scenario | Design Expectation | Test Steps | Observations | Matches Design? |
| -- | -------- | ------------------ | ---------- | ------------ | ---------------- |
| F1 | Filter crash (transient) | Pod failure should trigger controller to re-enqueue the file (attempts++) until success or maxAttempts; PipelineRun stays Progressing until completion. | Patched `pipeline-failure-sample` to `maxAttempts=3`, triggered run `pipeline-failure-sample-ceb1028e`, tailed Job logs to see filter exit. | `valkey-cli XREVRANGE pr:ceb1028e:work` showed re-enqueued entries with `attempts=1`; `XPENDING` returned 0 and PipelineRun remained `Progressing=True` until manual cleanup. | Yes |
| F2 | Bad input / permanent filter failure | After `maxAttempts`, controller should DLQ the file with reason and mark Progressing=false once all work is accounted; failed files reflected in status `counts.failed`. | Trigger `pipeline-failure-sample` (maxAttempts=1) via `curl -X POST .../pipeline-failure-sample/runs`; wait for Job to exhaust retries. | `kubectl get pipelinerun pipeline-failure-sample-4596b674 -o json` shows `CompletionTime`, `Degraded=True`, `counts.failed=3`; `valkey-cli XLEN pr:4596b674:dlq` returns `3` with `XRANGE` entries citing exit code 42. | Yes |
| F3 | Init container failure before claiming | Job restarts pod; no message in Valkey so no DLQ; PipelineRun remains Progressing. | Attempted to trigger by using missing S3 secret (`pipeline-init-missing-secret`), but HTTP API rejected run creation before controller involvement. | Scenario currently blocked by API pre-validation; need direct PipelineRun injection or API bypass to observe controller behavior. | Not verified |
| F4 | Init container failure after claim/download error | Controller should re-enqueue (attempts++) or DLQ based on maxAttempts; original message XACKed. | Ran `pipeline-slow` with valid creds, then patched `gcs-credentials` to invalid and deleted the Job so replacement pods used bad creds; claimer logged `failed to download file: Access denied.` after claiming. | Message re-enqueued only after `pendingTimeout` via reclaimer (`valkey-cli XREVRANGE pr:2c4ffb0f:work` showed `attempts=1` ~70 s later); immediate re-enqueue via annotations did not occur because claimer failed before patching pod annotations. | Partially (recovery works after timeout; immediate retry per design not observed) |
| F5 | Pod/node loss after claim (reclaimer) | Message sits pending until `pendingTimeout`; reclaimer XAUTOCLAIM → re-enqueue/DLQ; counts eventually stabilize. | Ran `pipeline-reclaim-test` (pendingTimeout=15s), deleted running pod via `kubectl delete pod ... --force` after claimer reported claim. | After ~20s, new pod claimed next file; `valkey-cli XREVRANGE pr:83c690b2:work` showed original files re-enqueued with `attempts=1`, confirming reclaimer moved them back to queue; `XPENDING` returned 0. | Yes |
| F6 | Image pull error | Pod stuck in `ImagePullBackOff`; controller should re-enqueue, retry up to `maxAttempts`, and degrade the run if failures persist. | Recreated `pipeline-bad-image` and triggered run `pipeline-bad-image-40be09d0` with the updated controller running locally. | Controller re-enqueues once per failure, then flushes the stream to DLQ on job failure: `XRANGE pr:40be09d0:dlq` shows one entry per file with `attempts=2`, and `XLEN pr:40be09d0:work` returns `0`; PipelineRun conditions flip to `Degraded` with `BackoffLimitExceeded`. | Yes |
| F7 | OOMKilled filter | Pod failure should follow retry/DLQ path similar to filter crash. | Triggered `pipeline-oom` (Python filter allocating 256 MiB with 64 Mi limit) run `pipeline-oom-76147f8f`. | Multiple pods exited `OOMKilled`; Job hit `BackoffLimitExceeded` and PipelineRun transitioned to `Degraded`, but `counts.failed` remained 0 and messages stayed queued in Valkey (no DLQ entries). | Partial (degraded condition set, but DLQ accounting missing) |
| F8 | Valkey outage | Init blocks on XREADGROUP; reconciliation pauses; once Valkey recovers, processing resumes without data loss. | Ran `pipeline-slow` run `pipeline-slow-9fe83364`, scaled `valkey` deployment to 0 during processing, then back to 1. | Claimer failures (`connect: connection refused`) while Valkey down caused Job to hit `BackoffLimitExceeded`; PipelineRun degraded when Valkey returned instead of resuming. | No (outage currently causes run failure rather than pause/resume) |
